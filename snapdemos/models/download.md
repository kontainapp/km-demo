# where to download these models from
https://huggingface.co/TheBloke/LLaMa-7B-GGML/tree/main


For example, the 7b q8 model is at the following link:
[llama-7b.ggmlv3.q8_0.bin](https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q8_0.bin)


```bash
# run this in this folder
wget https://huggingface.co/TheBloke/LLaMa-7B-GGML/resolve/main/llama-7b.ggmlv3.q8_0.bin
```

llama
```bash
wget https://huggingface.co/TheBloke/LLaMa-13B-GGML/resolve/main/llama-13b.ggmlv3.q8_0.bin
```