# description
This folder leverages the very recently created 'vllm' project that claims to be orders of magnitude faster than the huggingface transformers library by using PagedAttention

Reference:
- https://vllm.readthedocs.io/en/latest/

Please note that this works ONLY on machines with NVidia GPU and CUDA installed.

# supported models
vllm can use and run as inference server the following opensource LLM models:

- https://vllm.readthedocs.io/en/latest/models/supported_models.html
