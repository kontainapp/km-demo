# -*- coding: utf-8 -*-
"""simple-torch-neural-cuda.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AckpMkwWeL8mA-O0Uu3JsAbeldnggSrk
"""

# ref
# https://towardsdatascience.com/build-a-simple-neural-network-using-pytorch-38c55158028d
# !pip install torch

import torch
import torch.nn as nn

# device selection
device = (
    "cuda"
    if torch.cuda.is_available()
    else "mps"
    if torch.backends.mps.is_available()
    else "cpu"
)

print(f"Using {device} device")

# NOTE USE OF .to(device) for both data as well as model, else
# Error: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!
# 2. Now we will define variables like input size, hidden unit, output size, batch size, and the learning rate.
n_input, n_hidden, n_out, batch_size, learning_rate = 10, 15, 1, 100, 0.01

# We will now randomly initialize the dummy input and the output target data (or tensor)as follows:
data_x = torch.randn(batch_size, n_input).to(device)
data_y = (torch.rand(size=(batch_size, 1)) < 0.5).float().to(device)

# We initialized the input data with 100 data samples with 10 features each and respectively initialized the output data with 100 data points.
print(data_x.size())
print(data_y.size())

# 3. Define Neural Network Model
# Using in-built functions, we will create the simple sequential model with output sigmoid layer as follows:
model = nn.Sequential(nn.Linear(n_input, n_hidden),
                      nn.ReLU(),
                      nn.Linear(n_hidden, n_out),
                      nn.Sigmoid()).to(device)

# move to cuda
# model.to(device)
print(model)

# Next, we will define the loss function and the optimizer for gradient descent. 
# Under the nn package, there are several different loss function. 
# Here we will usenn.MSELoss as the loss function of the model which computes the mean-squared error between the input and the target. 
# Similarly, torch.optim package provides various optimization algorithms. 
# We will use stochastic gradient descent (SGD) optimizer.

loss_function = nn.MSELoss()
optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)

# 4. Training Loop
# Here, we will define the training loop with the following steps:
# Forward propagation — compute the predicted y and calculate the current loss
# Backward propagation — after each epoch we set the gradients to zero before starting to do backpropagation
# Gradient descent — Finally, we will update model parameters by calling optimizer.step() function
losses = []
for epoch in range(5000):
    pred_y = model(data_x)
    # pred_y = pred_y.to(device)
    # loss = loss_function(pred_y, data_y)
    # losses.append(loss.item())

    # model.zero_grad()
    # loss.backward()

    # optimizer.step()

# We can plot the loss and see how the model is training over each epoch.
import matplotlib.pyplot as plt

plt.plot(losses)
plt.ylabel('loss')
plt.xlabel('epoch')
plt.title("Learning rate %f"%(learning_rate))
# # plt.show()
plt.savefig('foo.png', bbox_inches='tight')

# We can see the loss is decreasing in each epoch which shows that the parameters are being learned.

# In this tutorial, you learned a step-by-step approach to developing a simple neural network model in PyTorch. 
# Specifically, you learned how to initialize random data, define the neural network model, and train them. 
# The major advantage of using PyTorch comes in twofold:

# The use of a tensor that provides a capability of operating NumPy-like array in GPU (Graphical Processing Unit).
# Availability of deep neural networks built on a tape-based automatic differentiation system.
# Congrats on building and training your first neural network with PyTorch!